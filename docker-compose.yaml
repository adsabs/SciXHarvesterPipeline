version: '3.1'
services:
  harvester_pipeline_main:
    build: .
    image: harvester_pipeline:latest
    networks:
      - kafka
    depends_on:
      - postgres
      - redis
      - kafka
      - schema-registry
      - minio
    container_name: harvester_pipeline_main
    environment:
      - GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=1
      - GRPC_PYTHON_BUILD_SYSTEM_ZLIB=1
    env_file:
      - aws_env_vars.env
    ports:
      - "50051:50051"
    volumes:
    - ./:/app
    - ./logs:/app/logs
    - ./scripts:/app/scripts

    stdin_open: true
    tty: true

  redis:
    container_name: redis
    networks:
      - kafka
    image: redis:7.0.5
    ports:
      - 6379:6379
    command: redis-server --save 20 1 --loglevel warning # --requirepass eYVX7EwVmmxKPCDmwMtyKVge8oLd2t81
    volumes: 
      - ./cache:/data

  postgres:
    container_name: postgres
    networks:
      - kafka
    image: postgres:14.4
    ports:
      - 5432:5432
    healthcheck:
      test: "pg_isready -U postgres -d harvester"
      interval: 2s
      timeout: 20s
      retries: 10
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=root
      - POSTGRES_DB=harvester
      - PGUSER=postgres
      - PGPASSWORD=root
      - PGDATABASE=harvester
    volumes:
      # Note that /docker-entrypoint-initdb.d will contain init-db-once.sql which initializes the DB only once
      # and sets some configuration needed for Kafka Connect
      - ./postgres:/docker-entrypoint-initdb.d

  
  pgadmin:
    container_name: pgadmin
    networks:
      - kafka
    image: dpage/pgadmin4:6.12
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - "5050:80"
    volumes:
      - pgadmin:/var/lib/pgadmin
  
  zookeeper:
    container_name: zookeeper
    networks:
      - kafka
    image: confluentinc/cp-zookeeper:7.2.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_ADMIN_ENABLE_SERVER: 'false'
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
  
  kafka:
    container_name: kafka
    networks:
      - kafka
    image: confluentinc/cp-enterprise-kafka:7.2.1
    depends_on: [zookeeper]
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      # Define "OUTSIDE:" to allow requests from outside the docker network (i.e., external clients using 'localhost')
      # Instead of "INTERNAL:" use "PLAINTEXT:" or the schema registry fails to connect (see https://github.com/confluentinc/schema-registry/issues/648#issuecomment-398032429)
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,OUTSIDE://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_BROKER_ID: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka
      
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      
      KAFKA_NUM_PARTITIONS: '12'
      KAFKA_COMPRESSION_TYPE: 'gzip'
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
    ports:
        - 9092:9092
        - 9094:9094
    volumes:
      - kafka:/var/lib/kafka/data
  
  ksql:
    image: confluentinc/ksqldb-server:0.27.2
    hostname: ksql
    container_name: ksql
    networks:
      - kafka
    depends_on:
      - kafka
      - connect
      - schema-registry
    ports:
      - "8088:8088"
    environment:
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_BOOTSTRAP_SERVERS: kafka:9092
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_NAME: "_ksql_processing_log"
      KSQL_KSQL_CONNECT_URL: http://connect:8083
  
  connect:
    container_name: connect
    networks:
      - kafka
    build: images/confluentinc-connect-custom/ # custom confluentinc/cp-kafka-connect:7.2.1 with extra plugins (e.g., JDBC, PostgreSQL sink)
    depends_on:
      - zookeeper
      - kafka
      - schema-registry
      - postgres
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONNECT_REST_PORT: '8083'
      CONNECT_REST_LISTENERS: 'http://0.0.0.0:8083'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'connect'
      CONNECT_CONFIG_STORAGE_TOPIC: '_connect_config'
      CONNECT_OFFSET_STORAGE_TOPIC: '_connect_offsets'
      CONNECT_STATUS_STORAGE_TOPIC: '_connect_statuses'
      CONNECT_GROUP_ID: 'kafka-connect'
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_KEY_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_VALUE_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components'
    ports:
      - 8083:8083
  
  schema-registry:
    container_name: schema-registry
    networks:
      - kafka
    image: confluentinc/cp-schema-registry:7.2.1
    environment:
     - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181
     - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=PLAINTEXT://kafka:9092
     - SCHEMA_REGISTRY_HOST_NAME=schema-registry
     - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081,http://localhost:8081
     - SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL=INFO
    depends_on: [zookeeper, kafka]
    ports:
      - 8081:8081
  
  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    networks:
      - kafka
    ports:
      - "8080:8080"
    restart: always
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_JMXPORT: 9101
      KAFKA_CLUSTERS_0_JMXSSL: "false"
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: first
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://connect:8083
      KAFKA_CLUSTERS_0_KSQLDBSERVER: http://ksql:8088
  
  akhq:
    container_name: akhq
    networks:
      - kafka
    image: tchiotludo/akhq:0.21.0
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka:9092"
              schema-registry:
                url: "http://schema-registry:8081"
              connect:
                - name: "connect"
                  url: "http://connect:8083"
    ports:
      - 9090:8080
    links:
      - kafka
      - schema-registry
  
  minio:
    networks:
      - kafka
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=supersecret
    image: quay.io/minio/minio:latest
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - ./data:/data
    restart: unless-stopped

networks:
  kafka:
    name: kafka
    driver: bridge

volumes:
  postgres:
    driver: local
  pgadmin:
    driver: local
  zookeeper-data:
    driver: local
  zookeeper-log:
    driver: local
  kafka:
    driver: local

