from confluent_kafka import Consumer
from confluent_kafka import Producer
from datetime import datetime
import time
import random
import logging
import json
import os
import imp
import inspect
import json
import ast
import sys

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager

sys.path.append("/app/")

import harvester_gRPC.gRPCHarvester.db as db

proj_home = os.path.realpath('../')
config = db.load_config(proj_home=proj_home)

def _consume_from_topic(consumer):
    logger.debug("Consuming from Harvester Topic")
    return consumer.poll()

class Harvester_APP:
    @contextmanager
    def session_scope(self):
        """Provide a transactional scope around a series of operations."""
        session = self.Session()
        try:
            yield session
            session.commit()
        except:
            session.rollback()
            raise
        finally:
            session.close()

    def __init__(self):
        self.engine = create_engine(config.get('SQLALCHEMY_URL'))
        self.Session = sessionmaker(self.engine)    

def Harvester_task(consumer, producer):
    while True:
        msg = _consume_from_topic(consumer)
        if msg:
            Finish = False
            tstamp = datetime.now()
            logger.debug("Received message {}".format(msg.value()))
            logger.info("Received message {}".format(json.loads(msg.value().decode('utf-8'))))
            logger.info("Job ID: {}".format(msg.key().decode('utf-8')))
            job_request = json.loads(msg.value().decode('utf-8'))
            job_request["hash"] = msg.key().decode('utf-8')
            job_request["status"] = "Processing"
            db.update_job_status(app, job_request["hash"], job_request["status"])
            logger.debug(b'This message was generated by the Harvester and was read from the gRPC topic %s.' % bytes(str(tstamp), 'utf-8'))
            for i in range(0,10):
                time.sleep(5)
                tstamp = datetime.now()
                if random.random() < 0.6 and b'Error' in msg.key(): 
                    job_request["status"] = "Error"
                    db.update_job_status(app, job_request)
                    logger.info(b'Error')
                    Finish = True
                    break
            if not Finish:
                job_request["status"] = "Done"
                db.update_job_status(app, job_request["hash"], status = "Done")                
                tstamp = datetime.now()            
                logger.info(b'Done %s.' % bytes(str(tstamp), 'utf-8'))
        else:
            logger.info("No new messages")
            time.sleep(2)
            continue

if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    logger=logging.getLogger(__name__)
    logger.info("Starting Harvester Service")
    logger.info(config)
    consumer = Consumer({'bootstrap.servers':config.get("KAFKA_BROKER"), 'auto.offset.reset':'latest', 'group.id': 'HarvesterPipeline1'})
    consumer.subscribe(['Harvester'])
    producer = Producer({'bootstrap.servers':config.get("KAFKA_BROKER")})
    app = Harvester_APP()
    Harvester_task(consumer, producer)

