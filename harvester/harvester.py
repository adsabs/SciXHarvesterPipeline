from datetime import datetime
import time
import logging
import json
import json
import sys
import redis
import boto3

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager

from harvester.metadata.arxiv_harvester import arxiv_harvesting
import harvester.s3_methods as s3_methods

sys.path.append("/app/")

import harvester.db as db

class Harvester_APP:
    @contextmanager
    def session_scope(self):
        """Provide a transactional scope around a series of operations."""
        session = self.Session()
        try:
            yield session
            session.commit()
        except:
            session.rollback()
            raise
        finally:
            session.close()

    def _consume_from_topic(self, consumer):
        self.logger.debug("Consuming from Harvester Topic")
        return consumer.poll()
    
    def _init_logger(self):
        logging.basicConfig(level=logging.DEBUG)
        self.logger=logging.getLogger(__name__)
        self.logger.info("Starting Harvester Service")
        self.logger.info(self.config)

    def __init__(self, config, start_s3=False):
        self.engine = create_engine(config.get('SQLALCHEMY_URL'))
        self.config = config
        self.logger = None
        self._init_logger()
        if start_s3:
            self.s3Client = s3_methods(boto3.client('s3'))
        else:
            self.s3Client = None
        self.Session = sessionmaker(self.engine)
        self.redis = redis.StrictRedis(config.get('REDIS_HOST', 'localhost'), config.get('REDIS_PORT', 6379), charset="utf-8", decode_responses=True) 
    

    def Harvester_task(self, consumer, producer):
        while True:
            msg = self._consume_from_topic(consumer)
            if msg:
                tstamp = datetime.now()
                self.logger.debug("Received message {}".format(msg.value()))
                job_request = msg.value()
                job_request["status"] = "Processing"
                db.update_job_status(self, job_request["hash"], job_request["status"])
                db.write_status_redis(self.redis, json.dumps({"job_id":job_request["hash"], "status":job_request["status"]}))
                self.logger.debug(b'This message was generated by the Harvester and was read from the gRPC topic %s.' % bytes(str(tstamp), 'utf-8'))
                
                if job_request.get("task") == "ARXIV":
                    job_request["status"] = arxiv_harvesting(self, job_request, producer)

                db.update_job_status(self, job_request["hash"], status = job_request["status"])
                db.write_status_redis(self.redis, json.dumps({"job_id":job_request["hash"], "status":job_request["status"]}))
                tstamp = datetime.now()            
                self.logger.info(b'Done %s.' % bytes(str(tstamp), 'utf-8'))

            else:
                self.logger.debug("No new messages")
                time.sleep(2)
                continue

    def _get_schema(self, schema_client):          
        try:
            avro_schema = schema_client.get_schema(self.config.get("SCHEMA_ID"))
            self.logger.info("Found schema: {}".format(avro_schema.schema_str))
        except Exception as e:
            avro_schema = None
            self.logger.warning("Could not retrieve avro schema with exception: {}".format(e))

        return avro_schema.schema_str