from unittest import TestCase
from harvester.harvester import Harvester_APP
from confluent_kafka.avro import AvroProducer
from mockschemaregistryclient import MockSchemaRegistryClient
import base
from harvester import utils, db
from mock import patch
from mockschemaregistryclient import MockSchemaRegistryClient
import harvester.metadata.arxiv_harvester as arxiv_harvester

class test_harvester(TestCase):
    def test_harvester_task(self):
        mock_job_request = base.mock_job_request()
        with base.base_utils.mock_multiple_targets({'get_schema': patch.object(utils, 'get_schema', return_value = '{"type":"record","name":"HarvesterMetadataOutput","fields":[{"name":"record_id","type":["string","null"],"doc":"The job ID generated by storing it in postgres.","logicalType":"uuid"},{"name":"record_xml","type":["string","null"],"doc":"The Harvested XML for a given record."},{"name":"task","type":{"type":"enum","name":"taskenum","symbols":["ARXIV","ELSEVIER","IOP","SIMBAD","MONITOR"]},"doc":"An enumerated type for specifying the harvester data source."}]}'), \
            'write_harvester_record': patch.object(db, 'write_harvester_record', return_value = True),\
            'arxiv_harvesting': patch.object(arxiv_harvester, 'arxiv_harvesting', return_value = "Success"), \
            'update_job_status': patch.object(db, 'update_job_status', return_value = True),\
            'write_status_redis': patch.object(db, 'write_status_redis', return_value = True)
            }) as mocked:
            mock_app = Harvester_APP(proj_home='tests/data')
            mock_app.schema_client = MockSchemaRegistryClient()
            producer = AvroProducer({}, schema_registry=mock_app.schema_client)
            mock_app.harvester_task(mock_job_request, producer)